{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass\n",
    "import os\n",
    "\n",
    "# 直接设置环境变量，或者用 getpass.getpass() 设置\n",
    "# os.environ['OPENAI_API_KEY'] = getpass.getpass()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LLM Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(base_url=\"https://api.deepseek.com\", model=\"deepseek-chat\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' LangSmith is a tool designed to help developers and language model practitioners to debug, evaluate, and understand the behavior of language models. While I don\\'t have specific information about a tool called \"LangSmith\" as of my last update, I can provide a general idea of how such a tool might help with testing language models:\\n\\n1. **Debugging**: LangSmith could provide a way to inspect the intermediate outputs of a language model, helping to identify where and why a model is producing incorrect or unexpected results. This is crucial for understanding the model\\'s reasoning process and for making targeted improvements.\\n\\n2. **Evaluation**: The tool might offer standardized metrics and benchmarks for evaluating language models. This includes metrics like perplexity, BLEU scores for machine translation, or ROUGE scores for summarization. By providing these metrics, LangSmith can help users assess the performance of their models against established standards.\\n\\n3. **Understanding Model Behavior**: LangSmith could include features that allow users to probe the model\\'s behavior on specific inputs or types of inputs. This could involve analyzing how the model responds to different prompts, identifying biases, or understanding the model\\'s strengths and weaknesses.\\n\\n4. **Test Case Management**: The tool might facilitate the creation and management of test cases. Users could define a set of inputs and expected outputs to systematically test the model\\'s performance across various scenarios.\\n\\n5. **Automated Testing**: LangSmith could integrate with continuous integration/continuous deployment (CI/CD) pipelines to automate the testing of language models. This ensures that any changes to the model or its environment are quickly and reliably tested.\\n\\n6. **Annotation and Feedback**: The tool might support human annotation, where human evaluators provide feedback on the model\\'s outputs. This can be particularly useful for tasks that require a high degree of nuance or domain-specific knowledge.\\n\\n7. **Comparison and A/B Testing**: Users could use LangSmith to compare different versions of a model or different models against each other. This can help in deciding which model performs best for a given task or dataset.\\n\\n8. **Documentation and Reporting**: The tool could generate reports and documentation that summarize the model\\'s performance, test results, and any issues found during testing. This is valuable for both internal development teams and external stakeholders.\\n\\nIf \"LangSmith\" is a specific tool or service, it would be best to refer to its official documentation or support resources for detailed information on how it can assist with testing language models. If it\\'s a hypothetical or new tool, the features described above are common functionalities that such a tool might offer to support the testing process.'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm.invoke(\"how can langsmith help with testing?\").content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a world class technical documentation writer.\"),\n",
    "    (\"user\", \"{input}\")\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptTemplate(input_variables=['input'], messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], template='You are a world class technical documentation writer.')), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], template='{input}'))])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptTemplate(input_variables=['input'], messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], template='You are a world class technical documentation writer.')), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], template='{input}'))])\n",
       "| ChatOpenAI(client=<openai.resources.chat.completions.Completions object at 0x7fd481853a90>, async_client=<openai.resources.chat.completions.AsyncCompletions object at 0x7fd4818791e0>, model_name='deepseek-chat', openai_api_key=SecretStr('**********'), openai_api_base='https://api.deepseek.com', openai_proxy='')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain = prompt | llm\n",
    "chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\" `pytest` 是一个非常流行的 Python 测试框架，它可以帮助你编写简单且可读性强的测试用例。以下是一些基本的使用方法：\\n\\n### 安装 `pytest`\\n\\n首先，你需要安装 `pytest`。如果你已经安装了 `pip`（Python 的包管理工具），那么可以通过以下命令安装：\\n\\n```bash\\npip install pytest\\n```\\n\\n### 编写测试用例\\n\\n`pytest` 会自动发现以 `test_` 开头或以 `_test` 结尾的文件，以及文件中以 `test` 开头的方法或函数。例如，你可以创建一个名为 `test_example.py` 的文件，内容如下：\\n\\n```python\\ndef add(x, y):\\n    return x + y\\n\\ndef test_add():\\n    assert add(2, 3) == 5\\n    assert add('a', 'b') == 'ab'\\n```\\n\\n### 运行测试\\n\\n在命令行中，进入包含测试文件的目录，然后运行以下命令：\\n\\n```bash\\npytest\\n```\\n\\n`pytest` 会自动运行所有发现的测试用例，并显示结果。如果所有测试都通过，你将看到类似以下的输出：\\n\\n```\\n============================= test session starts ==============================\\nplatform linux -- Python 3.x.y, pytest-6.x.y, py-1.x.y, pluggy-0.x.y\\nrootdir: /path/to/your/project\\ncollected 1 item\\n\\ntest_example.py .                                                         [100%]\\n\\n============================== 1 passed in 0.01s ===============================\\n```\\n\\n如果测试失败，`pytest` 会显示失败的原因和位置。\\n\\n### 高级用法\\n\\n`pytest` 还支持许多高级功能，例如：\\n\\n- **参数化测试**：使用 `pytest.mark.parametrize` 装饰器来运行同一个测试函数多次，每次使用不同的参数。\\n- **夹具（Fixtures）**：用于设置测试环境，可以在测试函数中作为参数使用。\\n- **测试夹具**：使用 `conftest.py` 文件来共享夹具。\\n- **跳过测试**：使用 `pytest.mark.skip` 或 `pytest.mark.skipif` 装饰器来跳过某些测试。\\n- **xfail**：使用 `pytest.mark.xfail` 装饰器来标记预期会失败的测试。\\n- **测试报告**：使用插件如 `pytest-html` 来生成 HTML 格式的测试报告。\\n\\n更多详细信息和高级用法，请参考 `pytest` 的官方文档：[https://docs.pytest.org/](https://docs.pytest.org/)。\""
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "message = chain.invoke({\"input\": \"Pytest 怎么使用?\"})\n",
    "message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " `pytest` 是一个非常流行的 Python 测试框架，它可以帮助你编写简单且可读性强的测试用例。以下是一些基本的使用方法：\n",
      "\n",
      "### 安装 `pytest`\n",
      "\n",
      "首先，你需要安装 `pytest`。如果你已经安装了 `pip`（Python 的包管理工具），那么可以通过以下命令安装：\n",
      "\n",
      "```bash\n",
      "pip install pytest\n",
      "```\n",
      "\n",
      "### 编写测试用例\n",
      "\n",
      "`pytest` 会自动发现以 `test_` 开头或以 `_test` 结尾的文件，以及文件中以 `test` 开头的方法或函数。例如，你可以创建一个名为 `test_example.py` 的文件，内容如下：\n",
      "\n",
      "```python\n",
      "def add(x, y):\n",
      "    return x + y\n",
      "\n",
      "def test_add():\n",
      "    assert add(2, 3) == 5\n",
      "    assert add('a', 'b') == 'ab'\n",
      "```\n",
      "\n",
      "### 运行测试\n",
      "\n",
      "在命令行中，进入包含测试文件的目录，然后运行以下命令：\n",
      "\n",
      "```bash\n",
      "pytest\n",
      "```\n",
      "\n",
      "`pytest` 会自动运行所有发现的测试用例，并显示结果。如果所有测试都通过，你将看到类似以下的输出：\n",
      "\n",
      "```\n",
      "============================= test session starts ==============================\n",
      "platform linux -- Python 3.x.y, pytest-6.x.y, py-1.x.y, pluggy-0.x.y\n",
      "rootdir: /path/to/your/project\n",
      "collected 1 item\n",
      "\n",
      "test_example.py .                                                         [100%]\n",
      "\n",
      "============================== 1 passed in 0.01s ===============================\n",
      "```\n",
      "\n",
      "如果测试失败，`pytest` 会显示失败的原因和位置。\n",
      "\n",
      "### 高级用法\n",
      "\n",
      "`pytest` 还支持许多高级功能，例如：\n",
      "\n",
      "- **参数化测试**：使用 `pytest.mark.parametrize` 装饰器来运行同一个测试函数多次，每次使用不同的参数。\n",
      "- **夹具（Fixtures）**：用于设置测试环境，可以在测试函数中作为参数使用。\n",
      "- **测试夹具**：使用 `conftest.py` 文件来共享夹具。\n",
      "- **跳过测试**：使用 `pytest.mark.skip` 或 `pytest.mark.skipif` 装饰器来跳过某些测试。\n",
      "- **xfail**：使用 `pytest.mark.xfail` 装饰器来标记预期会失败的测试。\n",
      "- **测试报告**：使用插件如 `pytest-html` 来生成 HTML 格式的测试报告。\n",
      "\n",
      "更多详细信息和高级用法，请参考 `pytest` 的官方文档：[https://docs.pytest.org/](https://docs.pytest.org/)。\n"
     ]
    }
   ],
   "source": [
    "print(message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "output_parser = StrOutputParser()\n",
    "\n",
    "chain = prompt | llm | output_parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " `pytest` 是一个非常流行的 Python 测试框架，它可以帮助你编写更简单、更可读的测试代码。以下是一些基本的 `pytest` 使用方法：\n",
      "\n",
      "### 安装 pytest\n",
      "\n",
      "首先，你需要安装 `pytest`。如果你已经安装了 `pip`（Python 的包管理工具），那么可以通过以下命令安装 `pytest`：\n",
      "\n",
      "```bash\n",
      "pip install pytest\n",
      "```\n",
      "\n",
      "### 编写测试用例\n",
      "\n",
      "`pytest` 会自动查找以 `test_` 开头或以 `_test` 结尾的函数，并将它们作为测试用例执行。例如，你可以创建一个名为 `test_example.py` 的文件，内容如下：\n",
      "\n",
      "```python\n",
      "def add(a, b):\n",
      "    return a + b\n",
      "\n",
      "def test_add():\n",
      "    assert add(2, 3) == 5\n",
      "    assert add(0, 0) == 0\n",
      "    assert add(-1, 1) == 0\n",
      "```\n",
      "\n",
      "### 运行测试\n",
      "\n",
      "在命令行中，进入包含测试文件的目录，然后运行以下命令：\n",
      "\n",
      "```bash\n",
      "pytest\n",
      "```\n",
      "\n",
      "`pytest` 会自动发现并运行所有的测试用例。如果所有的断言都通过了，那么测试就会成功，否则会失败。\n",
      "\n",
      "### 使用参数\n",
      "\n",
      "`pytest` 支持很多命令行参数，例如：\n",
      "\n",
      "- `-v` 或 `--verbose`：增加输出信息的详细程度。\n",
      "- `-k EXPRESSION`：只运行匹配表达式的测试。\n",
      "- `-x` 或 `--exitfirst`：当遇到第一个失败或错误时立即退出。\n",
      "- `--lf` 或 `--last-failed`：只重新运行上次运行失败的测试。\n",
      "- `--ff` 或 `--failed-first`：先运行上次失败的测试，然后运行所有其他的测试。\n",
      "\n",
      "### 使用 fixtures\n",
      "\n",
      "`pytest` 支持 fixtures，它们是测试函数可以使用的资源，例如数据库连接、外部服务客户端等。Fixtures 可以用来管理测试环境的设置和拆卸。\n",
      "\n",
      "例如，你可以定义一个名为 `my_fixture` 的 fixture：\n",
      "\n",
      "```python\n",
      "import pytest\n",
      "\n",
      "@pytest.fixture\n",
      "def my_fixture():\n",
      "    # 设置代码\n",
      "    setup_code = \"...\"\n",
      "    yield\n",
      "    # 拆卸代码\n",
      "    teardown_code = \"...\"\n",
      "```\n",
      "\n",
      "然后，你可以在测试函数中使用这个 fixture：\n",
      "\n",
      "```python\n",
      "def test_with_fixture(my_fixture):\n",
      "    # 测试代码\n",
      "    pass\n",
      "```\n",
      "\n",
      "### 使用插件\n",
      "\n",
      "`pytest` 有大量的插件可以扩展其功能。你可以通过 `pip` 安装这些插件，并在 `pytest` 配置文件中启用它们。\n",
      "\n",
      "例如，要安装 `pytest-cov` 插件来生成代码覆盖率报告，你可以运行：\n",
      "\n",
      "```bash\n",
      "pip install pytest-cov\n",
      "```\n",
      "\n",
      "然后，你可以使用以下命令运行测试并生成覆盖率报告：\n",
      "\n",
      "```bash\n",
      "pytest --cov=your_module_name\n",
      "```\n",
      "\n",
      "### 配置 pytest\n",
      "\n",
      "`pytest` 的配置可以通过 `pytest.ini`、`tox.ini` 或 `setup.cfg` 文件进行。这些文件通常位于项目的根目录下。\n",
      "\n",
      "例如，`pytest.ini` 文件可能包含以下内容：\n",
      "\n",
      "```ini\n",
      "[pytest]\n",
      "addopts = -v -s\n",
      "testpaths = tests\n",
      "python_files = test_*.py\n",
      "python_classes = Test*\n",
      "python_functions = test_*\n",
      "```\n",
      "\n",
      "这个配置指定了 `pytest` 应该在 `tests` 目录下查找测试文件，并且只运行以 `Test` 开头的类和以 `test_` 开头的函数。\n",
      "\n",
      "以上是 `pytest` 的一些基本用法。`pytest` 的功能非常强大，还有很多高级特性，例如参数化测试、xfail/skip 标记、测试类和模块级别的 setup/teardown 等。你可以通过阅读官方文档来了解更多信息：https://docs.pytest.org/en/latest/\n"
     ]
    }
   ],
   "source": [
    "result = chain.invoke({\"input\": \"Pytest 怎么使用?\"})\n",
    "\n",
    "# 直接输出的话，换行符是 \\n 而不是真的换行，不好看，所以还是 print 一下\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retrieval Chain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "使用 ollama 搭建本地模型：\n",
    "\n",
    "1. curl -fsSL https://ollama.com/install.sh | sh\n",
    "2. ollama pull mxbai-embed-large\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "loader = WebBaseLoader(\"https://docs.smith.langchain.com/user_guide\")\n",
    "\n",
    "docs = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.embeddings import OllamaEmbeddings\n",
    "\n",
    "embeddings = OllamaEmbeddings(model=\"mxbai-embed-large\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter()\n",
    "documents = text_splitter.split_documents(docs)\n",
    "vector = FAISS.from_documents(documents, embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(\"\"\"Answer the following question based only on the provided context:\n",
    "\n",
    "<context>\n",
    "{context}\n",
    "</context>\n",
    "\n",
    "Question: {input}\"\"\")\n",
    "\n",
    "document_chain = create_stuff_documents_chain(llm, prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.documents import Document\n",
    "\n",
    "result = document_chain.invoke({\n",
    "    \"input\": \"how can langsmith help with testing?\",\n",
    "    \"context\": [Document(page_content=\"langsmith can let you visualize test results\")]\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Based on the provided context, Langsmith can help with testing by allowing you to visualize test results. This visualization capability can provide insights into the performance and outcomes of tests, making it easier to analyze and understand the results.\n"
     ]
    }
   ],
   "source": [
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import create_retrieval_chain\n",
    "\n",
    "retriever = vector.as_retriever()\n",
    "retrieval_chain = create_retrieval_chain(retriever, document_chain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " LangSmith can help with testing in several ways:\n",
      "\n",
      "1. **Initial Test Set**: LangSmith allows developers to create datasets, which are collections of inputs and reference outputs, and use these to run tests on their LLM applications. These test cases can be uploaded in bulk, created on the fly, or exported from application traces. LangSmith also makes it easy to run custom evaluations (both LLM and heuristic based) to score test results.\n",
      "\n",
      "2. **Comparison View**: When prototyping different versions of your applications and making changes, LangSmith provides a user-friendly comparison view for test runs to track and diagnose regressions in test scores across multiple revisions of your application. This helps in understanding which variant is performing better.\n",
      "\n",
      "3. **Playground**: LangSmith provides a playground environment for rapid iteration and experimentation. Every playground run is logged in the system and can be used to create test cases or compare with other runs.\n",
      "\n",
      "4. **Beta Testing**: During beta testing, LangSmith supports feedback collection and run annotation, which are critical for understanding how the app is performing in real-world scenarios. It allows for attaching feedback scores to logged traces and filtering on traces that have a specific feedback tag and score. Annotating traces and adding runs to a dataset are also supported to refine and improve application performance.\n",
      "\n",
      "5. **Production**: In the production stage, LangSmith enables close inspection of key data points, growing benchmarking datasets, annotating traces, and drilling down into important data in trace view. It also provides online evaluations and automations to process and score production traces in near real-time.\n",
      "\n",
      "6. **Monitoring and A/B Testing**: LangSmith provides monitoring charts to track key metrics over time and allows for tag and metadata grouping for A/B testing changes in prompt, model, or retrieval strategy.\n",
      "\n",
      "7. **Automations**: Automations in LangSmith allow you to perform actions on traces in near real-time, such as automatically scoring traces, sending them to annotation queues, or sending them to datasets.\n",
      "\n",
      "8. **Threads**: LangSmith provides a threads view that groups traces from a single conversation together, making it easier to track the performance of your application across multiple turns, which is particularly useful for multi-turn applications.\n"
     ]
    }
   ],
   "source": [
    "response = retrieval_chain.invoke({\"input\": \"how can langsmith help with testing?\"})\n",
    "print(response[\"answer\"])\n",
    "\n",
    "# LangSmith offers several features that can help with testing:..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.12 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
